Batch gradient descent to train a binary logistic regression classifier using L2 regularization:

Load test and train data sets.
Normalize.
Batch loop:
	Loop over training examples:
		Calculate gradient vector of objective function with respect to weights:
			$\nabla L=\frac{1}{m}(X)^T\times (g(X\textbf{w})-y)+\frac{1}{m}\lambda \textbf{w}$
	Update weight vector:
		$\textbf{w} \leftarrow \textbf{w}-\alpha\nabla L$
		
$\alpha$ and $\lambda$ can be manipulated to optimize the model's accuracy and the training time required.
A learning rate of $\alpha =0.1$ and regularization parameter $10^{-3}\leq\lambda\leq10^3$ give accuracies of around 90 % after 10 iterations.